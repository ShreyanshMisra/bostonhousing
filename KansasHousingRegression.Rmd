---
title: "Multiple Linear Regression on Boston Housing Dataset"
author: ""
date: "`r Sys.Date()`"
output:
  pdf_document:
    number_sections: true
---

```{r include=FALSE, messages=FALSE}
library(knitr)
library(ggplot2)
library(caret)
library(ggplot2)
library(tidyr)
library(dplyr)
library(car)
library(lmtest)
library(nlme)
```

# Introduction

Multiple Linear Regression estimates the relationship between a dependent variable and multiple independent variables. The relationship between the dependent variable and independent variables is assumed to be linear. The equation takes the form $Y=\beta_0 + \beta_1 X_1 + \beta_2 X_2 + ... + \varepsilon$ where $\beta_0$ and $\beta_1$ are calculated such that the sum of the squares of the vertical differences between observed and predicted points are minimized. The equation is then used to predict values of $Y$ for given $X_n$ values.

This investigation focused on predicting the median value of homes in a Boston neighborhood based on 12 predictors. We used the Boston Housing dataset, sourced from the StatLib archive (http://lib.stat.cmu.edu/datasets/boston), which was compiled based on the 1970 U.S Census. It contains records from 506 unlabelled neighborhoods in Boston. 

R packages used and 

result / formula

# Data description

```{r}
housing.df <- read.csv("boston_house_prices.csv")
num_rows <- nrow(housing.df)
sum_missingdata <- sum(is.na(housing.df))

cat("Number of Rows: ", num_rows, "   Rows with Missing Data: ", sum_missingdata)
```

## Dataset

This dataset contains information about 506 neighborhoods in Boston, collected by the U.S Census Service in 1970 census. There are 506 records and 13 variables in the dataset. From an initial analysis, there were no missing data points.

```{r }
head(housing.df, 3)
```

## Variables
- `crim`: per capita crime rate by town
- `zn`: proportion of residential land zoned for lots over 25,000 sq.ft
- `indus`: proportion of non-retail business acres per town
- `chas`: Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)
- `nox`: nitric oxides concentration (parts per 10 million)
- `rm`: average number of rooms per dwelling
- `age`: proportion of owner-occupied units built prior to 1940
- `dis`: weighted distances to five Boston employment centres
- `rad`: index of accessibility to radial highways
- `tax`: full-value property-tax rate per USD 10,000
- `ptratio`: pupil-teacher ratio by town
- `lstat`: percentage of lower status of the population
- `medv`: median value of owner-occupied homes in USD 1000â€™s

We can identify `medv` as our dependent variable as the median value of homes in the neighborhood is what we are predicting. The remaining 12 variables are our independent variables.

## Outlier Detection

```{r }
housing_box <- housing.df %>%
  pivot_longer(cols = everything(), names_to = "variable", values_to = "value")

ggplot(housing_box, aes(x = "", y = value)) +
  geom_boxplot(fill = "steelblue") +
  theme_minimal() +
  facet_wrap(~ variable, scales = "free_y") +
  theme(axis.text.x = element_blank(), axis.ticks.x = element_blank())
```

# Analysis

## Train and Test Split

Our dataset was split into a training and testing set, where approximately 70% of the dataset is used for training and the remaining 30% is used for testing.  

```{r}
set.seed(123)

split <- 0.75

trainIndex <- createDataPartition(housing.df$medv, p = split)
trainIndex <- unlist(trainIndex)

train <- housing.df[trainIndex, ]
test <- housing.df[-trainIndex, ]

num_row_train <- nrow(train)
num_row_test <- nrow(test)

cat("Number of Rows in Train Set: ", num_row_train, "   Number of Rows in Test Set: ", num_row_test)
```

Specifically, 381 records were used for training and 125 were reserved for testing.

```{r }
train_control <- trainControl(method="LOOCV")
model <- train(medv ~ . , method="lm", data = train, trControl=train_control)
print(model$results)
```
```{r }
summary(model)
```
The variables `indus`, `age`, and `chas` have p-values of $0.809323$, $0.811548$, and $0.090986$ respectively, all of which are $>0.05$ making them insignificant predictors of `medv`. As they do not influence the median value of homes in Boston to an extent that can be deemed signficiant, we can remove them from the linear model. 

```{r}
model_significant <- train(medv ~ crim + zn + nox + rm + dis + rad + tax + ptratio + lstat,
      method="lm", 
      data = train, 
      trControl=train_control)

print(model_significant$results)
```

```{r}
train$predicted <- predict(model_significant, newdata = train)

# Scatter plot with regression line
ggplot(train, aes(x = medv, y = predicted)) +
  geom_point(alpha = 0.6) +  # Scatter plot
  geom_smooth(method = "lm", color = "blue", se = FALSE) +  # Best-fit line
  labs(title = "Actual vs. Predicted MEDV",
       x = "Actual MEDV",
       y = "Predicted MEDV") +
  theme_minimal()
```



## Assumptions of Multiple Linear Regression

1. Linearity - the relationship between the independent variables and dependent variable should be linear
2. Independence Of Errors - each data point's error should be independent of other points' errors (no observation should influence another)
3. Homoscedasticity - variance of the errors remains consistent across all values of the independent variable
4. Normality Of Errors - errors are normally distributed

### Assumption 1: Linearity

To Verify Assumption 1, we use a scatter plot to determine the general trend of the data. 

```{r}
par(mfrow = c(2, 2))

# Assumption 1
plot(model_significant$finalModel$fitted.values, residuals(model_significant$finalModel),
     main="Residuals vs Fitted Values",
     xlab="Fitted Values", ylab="Residuals",
     pch=20, col="blue")
abline(h=0, col="red", lwd=2)

# Assumption 2
plot(residuals(model_significant$finalModel))
abline(h=0, col = 'Red')

# Assumption 4
qqnorm(residuals(model_significant$finalModel), main="Normal Q-Q Plot")
qqline(residuals(model_significant$finalModel), col="red")

histogram(residuals(model_significant$finalModel), prob = TRUE)
```
This residual vs Fitted values graph displays a random pattern with a red line at 0, suggesting the linearity is valid

For Assumption 2 we plot the residuals

```{r}
dwtest(model_significant$finalModel)
```

For assumption 3 we will look at the levene test

```{r}
train$FittedGroup <- ifelse(model_significant$finalModel$fitted.values < 
                            median(model_significant$finalModel$fitted.values), 
                            "Low", "High")

# Apply Levene's Test correctly on training data
leveneTest(residuals(model_significant$finalModel) ~ train$FittedGroup)

```
Since the p value is 0.2079 which is greater than 0.05, we fail to reject the null hypothesis which suggests the assumption of homscedasticity holds

For assumption 4, we will create a QQ plot and a histogram of our residuals


In regards to the QQ plot we want the residuals to follow the red line in which they do, furthermore in regards to the histogram we want it to form an approximate normal distribution which it also does. We can confirm our findings with a shapiro test:

```{r}
shapiro.test(residuals(model_significant$finalModel))
```

# Model Evaluation and Prediction

You should base on the training set you should hav gotten a model in (3), then you must use subset selection and all necessary test to verify the final model that you get is the best model (Model assessment and model accuracy) and use it to make a prediction

```{r }
predictions <- predict(model_significant, newdata = test)
actual <- test$medv

mae <- mean(abs(predictions - actual)) # MAE
mse <- mean((predictions - actual)^2) # MSE
rmse <- sqrt(mse) # RMSE

mae
mse
rmse
```

# Conclusion

The summary your all your work and results in this part and point out positive side of your model, negative side of your model and possible future work or any factors that affect your model accuracy.


# References
- https://www.cs.toronto.edu/~delve/data/boston/bostonDetail.html
- https://www.sthda.com/english/articles/40-regression-analysis/168-multiple-linear-regression-in-r/